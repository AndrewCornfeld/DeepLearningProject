{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "53bdc264-7cce-4a2d-bc62-88a70d720a8b",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2025-04-10T16:58:14.178677Z",
          "start_time": "2025-04-10T16:58:13.603146Z"
        },
        "id": "53bdc264-7cce-4a2d-bc62-88a70d720a8b"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"shashwatwork/knee-osteoarthritis-dataset-with-severity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dd989d43-bbfc-47c4-852b-5d0a2f28806c",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2025-04-10T16:58:14.185927Z",
          "start_time": "2025-04-10T16:58:14.178262Z"
        },
        "id": "dd989d43-bbfc-47c4-852b-5d0a2f28806c"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(path, \"train\")\n",
        "val_dir = os.path.join(path, \"val\")\n",
        "test_dir = os.path.join(path, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5778 files belonging to 5 classes.\n",
            "Found 826 files belonging to 5 classes.\n",
            "Found 1656 files belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "batch_size = 32\n",
        "img_size = (224, 224)\n",
        "num_classes = 5\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomTranslation(0.1, 0.1)\n",
        "])\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess(x, y):\n",
        "    return preprocess_input(x), y\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "val_dataset = val_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "test_dataset = test_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-10T16:58:20.129377Z",
          "start_time": "2025-04-10T16:58:16.145709Z"
        },
        "id": "de39e12d472bcbb2",
        "outputId": "60b63d01-3564-4947-d982-af9d387f2153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "de39e12d472bcbb2",
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo4co9KzM6zv",
        "outputId": "61aef4fe-d660-4343-9659-807e3238b0a1"
      },
      "id": "Xo4co9KzM6zv",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras_tuner) (4.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e64b05-3be1-4a7a-976f-04b816b60852",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2025-04-10T16:59:02.922545Z",
          "start_time": "2025-04-10T16:58:50.291238Z"
        },
        "id": "d4e64b05-3be1-4a7a-976f-04b816b60852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39cda2e-23b4-40db-9fb4-cd8a8b132f3a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "384               |384               |dense_units\n",
            "leaky_relu        |leaky_relu        |activation\n",
            "0.55              |0.55              |dropout_rate\n",
            "0.00088181        |0.00088181        |learning_rate\n",
            "3                 |3                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    base_model1 = ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    base_model1.trainable = True  # Fine-tune the base\n",
        "\n",
        "    model1 = models.Sequential()\n",
        "    model1.add(base_model1)\n",
        "    model1.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Tune number of units in Dense layer\n",
        "    hp_units_1 = hp.Int('dense_units', min_value=128, max_value=512, step=64)\n",
        "\n",
        "    # Tune activation function\n",
        "    activation = hp.Choice('activation', ['relu', 'leaky_relu'])\n",
        "    if activation == 'leaky_relu':\n",
        "        model1.add(layers.Dense(hp_units_1))\n",
        "        model1.add(layers.LeakyReLU())\n",
        "    else:\n",
        "        model1.add(layers.Dense(hp_units_1, activation='relu'))\n",
        "\n",
        "    # Tune dropout rate\n",
        "    hp_dropout = hp.Float('dropout_rate', 0.3, 0.75, step=0.05)\n",
        "    model1.add(layers.Dropout(hp_dropout))\n",
        "\n",
        "    model1.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n",
        "\n",
        "    model1.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model1\n",
        "\n",
        "# Set up tuner\n",
        "tuner1 = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='kt_dir',\n",
        "    project_name='resnet_tuning'\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "\n",
        "# Search for best hyperparameters\n",
        "tuner1.search(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")\n",
        "\n",
        "# Retrieve and train best model\n",
        "best_hp = tuner1.get_best_hyperparameters(1)[0]\n",
        "model1 = tuner1.hypermodel.build(best_hp)\n",
        "\n",
        "history = model1.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp1 = tuner1.get_best_hyperparameters(1)[0]\n",
        "model1 = tuner1.hypermodel.build(best_hp1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history1 = model1.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")"
      ],
      "metadata": {
        "id": "Y_36dCdyT-xY"
      },
      "id": "Y_36dCdyT-xY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss, accuracy = model1.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "3Dm0xM2RWHNY"
      },
      "id": "3Dm0xM2RWHNY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    base_model2 = DenseNet121(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    base_model2.trainable = True\n",
        "\n",
        "    model2 = models.Sequential()\n",
        "    model2.add(base_model2)\n",
        "    model2.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Tune number of units in Dense layer\n",
        "    hp_units2 = hp.Int('dense_units', min_value=128, max_value=512, step=64)\n",
        "\n",
        "    # Tune activation function\n",
        "    activation = hp.Choice('activation', ['relu', 'leaky_relu'])\n",
        "    if activation == 'leaky_relu':\n",
        "        model2.add(layers.Dense(hp_units2))\n",
        "        model2.add(layers.LeakyReLU())\n",
        "    else:\n",
        "        model2.add(layers.Dense(hp_units2, activation='relu'))\n",
        "\n",
        "    # Tune dropout rate\n",
        "    hp_dropout = hp.Float('dropout_rate', 0.3, 0.75, step=0.05)\n",
        "    model2.add(layers.Dropout(hp_dropout))\n",
        "\n",
        "    model2.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n",
        "\n",
        "    model2.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model2\n",
        "\n",
        "# Set up the tuner\n",
        "tuner2 = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='kt_densenet',\n",
        "    project_name='densenet_tuning'\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner2.search(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")\n"
      ],
      "metadata": {
        "id": "9fAL8IpuJ7Vd"
      },
      "id": "9fAL8IpuJ7Vd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve and train the best model\n",
        "best_hp2 = tuner2.get_best_hyperparameters(1)[0]\n",
        "model2 = tuner2.hypermodel.build(best_hp2)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history2 = model2.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")"
      ],
      "metadata": {
        "id": "OXJzPlosthOt"
      },
      "id": "OXJzPlosthOt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss, accuracy = model2.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "WYvsAr5CWSDL"
      },
      "id": "WYvsAr5CWSDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    base_model_vgg = VGG16(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    base_model_vgg.trainable = True\n",
        "\n",
        "    model_vgg = models.Sequential()\n",
        "    model_vgg.add(base_model_vgg)\n",
        "    model_vgg.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Tune number of units in Dense layer\n",
        "    hp_units_vgg = hp.Int('dense_units', min_value=128, max_value=512, step=64)\n",
        "\n",
        "    # Tune activation function\n",
        "    activation = hp.Choice('activation', ['relu', 'leaky_relu'])\n",
        "    if activation == 'leaky_relu':\n",
        "        model_vgg.add(layers.Dense(hp_units_vgg))\n",
        "        model_vgg.add(layers.LeakyReLU())\n",
        "    else:\n",
        "        model_vgg.add(layers.Dense(hp_units_vgg, activation='relu'))\n",
        "\n",
        "    # Tune dropout rate\n",
        "    hp_dropout = hp.Float('dropout_rate', 0.3, 0.75, step=0.05)\n",
        "    model_vgg.add(layers.Dropout(hp_dropout))\n",
        "\n",
        "    model_vgg.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n",
        "\n",
        "    model_vgg.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model_vgg\n",
        "\n",
        "# Set up the tuner\n",
        "tuner_vgg = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='kt_vgg16',\n",
        "    project_name='vgg16_tuning'\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop_vgg = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "\n",
        "# Run the search\n",
        "tuner_vgg.search(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")\n"
      ],
      "metadata": {
        "id": "-14dXxmaW2TH"
      },
      "id": "-14dXxmaW2TH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Retrieve and train the best model\n",
        "best_hp_vgg = tuner_vgg.get_best_hyperparameters(1)[0]\n",
        "model_vgg = tuner_vgg.hypermodel.build(best_hp_vgg)\n",
        "\n",
        "history_vgg = model_vgg.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")"
      ],
      "metadata": {
        "id": "iVTUI0CVuEIC"
      },
      "id": "iVTUI0CVuEIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss, accuracy = model_vgg.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "8VvlkRwdW3oN"
      },
      "id": "8VvlkRwdW3oN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    base_model_efficientnet = EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    base_model_efficientnet.trainable = True\n",
        "\n",
        "    model_efficientnet = models.Sequential()\n",
        "    model_efficientnet.add(base_model_efficientnet)\n",
        "    model_efficientnet.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Tune number of units in Dense layer\n",
        "    hp_units_efficientnet = hp.Int('dense_units', min_value=128, max_value=512, step=64)\n",
        "\n",
        "    # Tune activation function\n",
        "    activation = hp.Choice('activation', ['relu', 'leaky_relu'])\n",
        "    if activation == 'leaky_relu':\n",
        "        model_efficientnet.add(layers.Dense(hp_units_efficientnet))\n",
        "        model_efficientnet.add(layers.LeakyReLU())\n",
        "    else:\n",
        "        model_efficientnet.add(layers.Dense(hp_units_efficientnet, activation='relu'))\n",
        "\n",
        "    # Tune dropout rate\n",
        "    hp_dropout = hp.Float('dropout_rate', 0.3, 0.75, step=0.05)\n",
        "    model_efficientnet.add(layers.Dropout(hp_dropout))\n",
        "\n",
        "    model_efficientnet.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    # Tune learning rate\n",
        "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n",
        "\n",
        "    model_efficientnet.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model_efficientnet\n",
        "\n",
        "# Set up the tuner\n",
        "tuner_efficientnet = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='kt_efficientnet',\n",
        "    project_name='efficientnet_tuning'\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "\n",
        "# Run the search\n",
        "tuner_efficientnet.search(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")\n",
        "\n",
        "# Retrieve and train the best model\n",
        "best_hp_efficientnet = tuner_efficientnet.get_best_hyperparameters(1)[0]\n",
        "model_efficientnet = tuner_efficientnet.hypermodel.build(best_hp)\n",
        "\n",
        "history_efficientnet = model_efficientnet.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, lr_schedule]\n",
        ")\n"
      ],
      "metadata": {
        "id": "h-Hrc5geyi_d"
      },
      "id": "h-Hrc5geyi_d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss, accuracy = model_efficientnet.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Vuaa7dyWYE-o"
      },
      "id": "Vuaa7dyWYE-o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "predictions1 = model1.predict(test_dataset)\n",
        "predictions2 = model2.predict(test_dataset)\n",
        "predictions_vgg = model_vgg.predict(test_dataset)\n",
        "predictions_efficientnet = model_efficientnet.predict(test_dataset)\n",
        "\n",
        "\n",
        "y_pred1 = np.argmax(predictions1, axis=1)\n",
        "y_pred2 = np.argmax(predictions2, axis=1)\n",
        "y_pred_vgg = np.argmax(predictions_vgg, axis=1)\n",
        "y_pred_efficientnet = np.argmax(predictions_efficientnet, axis=1)\n",
        "\n",
        "\n",
        "X_test_stacked = np.column_stack((y_pred1, y_pred2, y_pred_vgg, y_pred_efficientnet))\n",
        "y_test = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "\n",
        "estimators = [\n",
        "    ('model1', model1),\n",
        "    ('model2', model2),\n",
        "    ('model_vgg', model_vgg),\n",
        "    ('model_efficientnet', model_efficientnet)\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "\n",
        "stacking_model.fit(X_test_stacked, y_test)\n",
        "\n",
        "stacking_predictions = stacking_model.predict(X_test_stacked)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, stacking_predictions)\n",
        "print(f\"Stacking Model Accuracy on Test Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "K4JvxYjBMGpf"
      },
      "id": "K4JvxYjBMGpf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a15157-c1fc-4412-8d8a-012073ee829b",
      "metadata": {
        "id": "77a15157-c1fc-4412-8d8a-012073ee829b"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a graph of validation loss vs training loss\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'history' object contains the training history from model.fit\n",
        "# history = model.fit(...)\n",
        "\n",
        "# Access training and validation loss values\n",
        "training_loss = model1.history.history['loss']\n",
        "validation_loss = model1.history.history['val_loss']\n",
        "\n",
        "# Create the plot\n",
        "epochs = range(1, len(training_loss) + 1)\n",
        "plt.plot(epochs, training_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}